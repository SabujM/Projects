{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: bs4 in ./.local/lib/python3.7/site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/site-packages (from bs4) (4.8.2)\n",
      "Requirement already satisfied: soupsieve>=1.2 in /usr/local/lib/python3.7/site-packages (from beautifulsoup4->bs4) (2.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/site-packages (2.23.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/site-packages (from requests) (1.25.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests) (2019.11.28)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests) (2.9)\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: html5lib in /usr/local/lib/python3.7/site-packages (1.0.1)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/site-packages (from html5lib) (0.5.1)\n",
      "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/site-packages (from html5lib) (1.14.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4\n",
    "!pip install requests\n",
    "!pip install html5lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Scrape the details of most viewed videos on YouTube from Wikipedia:\n",
    "Url = https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos/ You need to find following details:\n",
    "A) Rank\n",
    "B) Name\n",
    "C) Artist\n",
    "D) Upload date\n",
    "E) Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "page1=requests.get(\"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup=BeautifulSoup(page1.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>List of most-viewed YouTube videos - Wikipedia</title>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank=[]\n",
    "name=[]\n",
    "artist=[]\n",
    "views=[]\n",
    "udate=[]\n",
    "trs=soup.find_all(\"tr\")\n",
    "trs_=trs[1:31]\n",
    "for tr in trs_:\n",
    "    i=tr.find_all(\"td\")\n",
    "    rank.append(i[0].text.replace(\".\", \"\"))\n",
    "    j=i[1].find(\"a\")\n",
    "    name.append(j.text)\n",
    "    artist.append(i[2].text)\n",
    "    views.append(i[3].text)\n",
    "    udate.append(i[4].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos[\"Rank\"]=rank\n",
    "videos[\"Name\"]=name\n",
    "videos[\"Artist\"]=artist\n",
    "videos[\"Views (In Billions)\"]=views\n",
    "videos[\"Upload Date\"]=udate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos.to_csv(\"Top Most Viewed Videos in YouTube from Wiki.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Scrape the details team India’s international fixtures from bcci.tv. Url = https://www.bcci.tv/.\n",
    "You need to find following details:\n",
    "A) Match title (I.e. 1st ODI)\n",
    "B) Series\n",
    "C) Place\n",
    "D) Date\n",
    "E) Time\n",
    "Note: - From bcci.tv home page you have reach to the international fixture page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "page2=requests.get(\"https://www.bcci.tv/international/fixtures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup=BeautifulSoup(page2.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>The Board of Control for Cricket in India</title>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "formt=[]\n",
    "match=[]\n",
    "formats=soup.find_all(\"div\", class_=\"fixture__format-strip\")\n",
    "for i in formats:\n",
    "    j=i.text.split()\n",
    "    formt.append(j[0])\n",
    "    match.append(\" \".join(j[1:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "date=[]\n",
    "time=[]\n",
    "place=[]\n",
    "match_title=[]\n",
    "fix=soup.find_all(\"div\", class_=\"fixture__description u-unskewed-text\")\n",
    "for i in fix:\n",
    "    j=i.text.split()\n",
    "    date.append(\" \".join(j[0:3]))\n",
    "    time.append(\" \".join(j[3:5]))\n",
    "    place.append(\" \".join(j[7:]))\n",
    "    match_title.append(\" \".join(j[5:7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixture=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixture[\"Match Title\"]=match_title\n",
    "fixture[\"Series\"]=match\n",
    "fixture[\"Place\"]=place\n",
    "fixture[\"Time\"]=time\n",
    "fixture[\"Date\"]=date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixture.to_csv(\"International Fixture of India in 2021.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Scrape the details of State-wise GDP of India from statisticstime.com. Url = http://statisticstimes.com/\n",
    "You have to find following details:\n",
    "A) Rank\n",
    "B) State\n",
    "C) GSDP(18-19)\n",
    "D) GSDP(17-18)\n",
    "E) Share(2017)\n",
    "F) GDP($ billion)\n",
    "Note: - From statisticstimes home page you have to reach to economy page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "page3=requests.get(\"http://statisticstimes.com/economy/india/indian-states-gdp.php\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup=BeautifulSoup(page3.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>GDP of Indian states - StatisticsTimes.com</title>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank=[]\n",
    "State=[]\n",
    "GSDP_19_20=[]\n",
    "GSDP_18_19=[]\n",
    "Share_=[]\n",
    "GDP=[]\n",
    "t=soup.find_all(\"tbody\")\n",
    "for i in t:\n",
    "    trs=i.find_all(\"tr\")\n",
    "    for j in trs:\n",
    "        k=j.find_all(\"td\")\n",
    "        rank.append(k[0].text)\n",
    "        State.append(k[1].text)\n",
    "        GSDP_19_20.append(k[2].text)\n",
    "        GSDP_18_19.append(k[3].text)\n",
    "        Share_.append(k[4].text)\n",
    "        GDP.append(k[5].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdps=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdps[\"Rank\"]=rank\n",
    "gdps[\"State\"]=State\n",
    "gdps[\"GSDP(19-20)\"]=GSDP_19_20\n",
    "gdps[\"GSDP(18-19)\"]=GSDP_18_19\n",
    "gdps[\"Share(2019)\"]=Share_\n",
    "gdps[\"GDP\"]=GDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdps.to_csv(\"State-wise GDP.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Scrape the details of trending repositories on Github.com. Url = https://github.com/\n",
    "You have to find the following details:\n",
    "A) Repository title\n",
    "B) Repository description\n",
    "C) Contributors count\n",
    "D) Language used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "page5=requests.get(\"https://github.com/trending\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup=BeautifulSoup(page5.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>Trending  repositories on GitHub today · GitHub</title>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "title=[]\n",
    "heads=soup.find_all(\"h1\", class_=\"h3 lh-condensed\")\n",
    "for i in heads:\n",
    "    j=i.text.split()\n",
    "    title.append(\" \".join(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "des=[]\n",
    "a=soup.find_all(\"article\", class_=\"Box-row\")\n",
    "for i in a:\n",
    "        j=i.find(\"p\", class_=\"col-9 text-gray my-1 pr-4\")\n",
    "        try:\n",
    "            des.append(\" \".join(j.text.split()))\n",
    "        except:\n",
    "            des.append(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "language=[]\n",
    "a=soup.find_all(\"div\", class_=\"f6 text-gray mt-2\")\n",
    "for i in a:\n",
    "        j=i.find(\"span\", itemprop=\"programmingLanguage\")\n",
    "        try:\n",
    "            language.append(j.text)\n",
    "        except:\n",
    "            language.append(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts=[]\n",
    "contri=soup.find_all(\"span\", class_=\"d-inline-block mr-3\")\n",
    "for i in contri:\n",
    "    j=i.find_all(\"a\")\n",
    "    counts.append(len(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "github=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "github[\"Repository Title\"]=title\n",
    "github[\"Description\"]=des\n",
    "github[\"Language Used\"]=language\n",
    "github[\"No of Contributors\"]=counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "github.to_csv(\"Trending Repositories.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Scrape the details of top 100 songs on billiboard.com.\n",
    "Url = https://www.billiboard.com/\n",
    "You have to find the following details:\n",
    "A) Song name\n",
    "B) Artist name\n",
    "C) Last week rank\n",
    "D) Peak rank\n",
    "E) Weeks on board\n",
    "Note: - From the home page you have to click on the charts option then hot 100-page link through code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Scrape the details of Data science recruiters from naukri.com.\n",
    "Url = https://www.naukri.com/\n",
    "You have to find the following details:\n",
    "A) Name\n",
    "B) Designation\n",
    "C) Company\n",
    "D) Skills they hire for\n",
    "E) Location\n",
    "Note: - From naukri.com homepage click on the recruiters option and the on the search pane type Data science and\n",
    "click on search. All this should be done through code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "page7=requests.get(\"https://www.naukri.com/data-science-jobs?k=data%20science\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup=BeautifulSoup(page7.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(soup.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name=[]\n",
    "n=soup.find_all(\"div\", class_=\"info fleft\")\n",
    "n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Scrape the details of Highest selling novels.\n",
    "Url = https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-greycompare/\n",
    "You have to find the following details:\n",
    "A) Book name\n",
    "B) Author name\n",
    "C) Volumes sold\n",
    "D) Publisher\n",
    "E) Genre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's showing....Sorry – we haven’t been able to serve the page you asked for.You may have followed an outdated link, or have mistyped a URL. If you believe this to be an error please report it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Scrape the details most watched tv series of all time from imdb.com.\n",
    "Url = https://www.imdb.com/list/ls095964455/\n",
    "You have to find the following details:\n",
    "A) Name\n",
    "B) Year span\n",
    "C) Genre\n",
    "D) Run time\n",
    "E) Ratings\n",
    "F) Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "page8=requests.get(\"https://www.imdb.com/list/ls095964455/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup=BeautifulSoup(page8.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>Top 100 most watched tv shows of all time - IMDb</title>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank=[]\n",
    "name=[]\n",
    "span=[]\n",
    "names=soup.find_all(\"div\", class_=\"lister-item-content\")\n",
    "for i in n:\n",
    "    j=i.find_all(\"h3\", class_=\"lister-item-header\")\n",
    "    for k in j:\n",
    "        rank.append(k.find(\"span\", class_=\"lister-item-index unbold text-primary\").text.replace(\".\",\"\"))\n",
    "        name.append(\" \".join(k.find(\"a\").text.split()))\n",
    "        span.append(k.find(\"span\", class_=\"lister-item-year text-muted unbold\").text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "run=[]\n",
    "runtime=soup.find_all(\"span\", class_=\"runtime\")\n",
    "for r in runtime:\n",
    "    run.append(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen=[]\n",
    "gens=soup.find_all(\"span\", class_=\"genre\")\n",
    "for r in gens:\n",
    "    gen.append(\" \".join(r.text.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings=[]\n",
    "rats=soup.find_all(\"div\", class_=\"ipl-rating-widget\")\n",
    "for i in rats:\n",
    "    ratings.append(i.text.split()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote=[]\n",
    "votes=soup.find_all(\"p\", class_=\"text-muted text-small\")\n",
    "for i in votes:\n",
    "    j=i.find_all(\"span\", {\"name\": \"nv\"})\n",
    "    for k in j:\n",
    "        vote.append(k.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "series=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "series['Name']=name\n",
    "series['Year span']=span\n",
    "series['Genre']=gen\n",
    "series['Run time']=run\n",
    "series['Ratings']=ratings\n",
    "series['Votes']=vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "series.to_csv(\"Top 100 Series.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Details of Datasets from UCI machine learning repositories.\n",
    "Url = https://archive.ics.uci.edu/\n",
    "You have to find the following details:\n",
    "A) Dataset name\n",
    "B) Data type\n",
    "C) Task\n",
    "D) Attribute type\n",
    "E) No of instances\n",
    "F) No of attribute\n",
    "G) Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "page10=requests.get(\"https://archive.ics.uci.edu/ml/datasets.php\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    }
   ],
   "source": [
    "soup=BeautifulSoup(page10.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>UCI Machine Learning Repository: Data Sets</title>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "tds=soup.find_all(\"td\", {\"valign\": \"top\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tds:\n",
    "    tables=i.find_all(\"table\", {\"border\":\"1\", \"cellpadding\":5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tables:\n",
    "    trs=i.find_all(\"tr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_trs=[]\n",
    "for i in range(len(trs)):\n",
    "        if i%2!=0:\n",
    "          imp_trs.append(trs[i])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "name=[]\n",
    "typ=[]\n",
    "task=[]\n",
    "att=[]\n",
    "ins=[]\n",
    "att2=[]\n",
    "year=[]\n",
    "for i in imp_trs:\n",
    "    ps=i.find_all(\"p\", class_=\"normal\")\n",
    "    name.append(ps[0].text)\n",
    "    typ.append(ps[1].text)\n",
    "    task.append(ps[2].text)\n",
    "    att.append(ps[3].text)\n",
    "    ins.append(ps[4].text)\n",
    "    att2.append(ps[5].text)\n",
    "    year.append(ps[6].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(559, 559, 559, 559, 559, 559, 559)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(name), len(typ),len(task), len(att), len(ins), len(att2), len(year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo[\"Dataset Name\"]=name\n",
    "repo[\"Type\"]=typ\n",
    "repo[\"Task\"]=task\n",
    "repo[\"Attributes Type\"]=att\n",
    "repo[\"Instances\"]=ins\n",
    "repo[\"Attributes\"]=att2\n",
    "repo[\"Year\"]=year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo.to_csv(\"Datasets from UCI.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Scrape the details of selenium exception from guru99.com. Url = https://www.guru99.com/\n",
    "You need to find following details:\n",
    "A) Name\n",
    "B) Description\n",
    "Note: - From guru99 home page you have to reach to selenium exception handling page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "page3=requests.get(\"https://www.guru99.com/exception-handling-selenium.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup=BeautifulSoup(page3.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables=soup.find_all(\"table\", class_=\"table table-striped\")\n",
    "for i in tables:\n",
    "    trs=i.find_all(\"tr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "name=[]\n",
    "des=[]\n",
    "for j in trs:\n",
    "    tds=j.find_all(\"td\")\n",
    "    name.append(tds[0].text)\n",
    "    des.append(tds[1].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "exceptions=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "exceptions[\"Name\"]=name[1:]\n",
    "exceptions[\"Description\"]=des[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "exceptions.to_csv(\"Details of Selenium Exceptions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
